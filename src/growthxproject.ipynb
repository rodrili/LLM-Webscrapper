{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "import torch\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_company_site(url):\n",
    "\n",
    "    \"\"\"\n",
    "    This function reads the content of a company's website and extracts the title, meta description, headings, and paragraphs.\n",
    "    Args:\n",
    "        url (str): The URL of the company's website.\n",
    "    Returns:\n",
    "        dict: A dictionary containing the extracted data.\n",
    "    \"\"\"\n",
    "    response = requests.get(url, timeout=5)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Extract various parts of the webpage\n",
    "    title = soup.find('title').get_text(strip=True) if soup.find('title') else \"No title found\"\n",
    "    meta_description = soup.find('meta', attrs={'name': 'description'})\n",
    "    meta_description = meta_description['content'].strip() if meta_description else \"No description found\"\n",
    "    headings = [h.get_text(strip=True) for h in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])]\n",
    "    paragraphs = [p.get_text(strip=True) for p in soup.find_all('p')]\n",
    "\n",
    "    # Combine extracted parts into a dictionary\n",
    "    extracted_data = {\n",
    "        \"title\": title,\n",
    "        \"meta_description\": meta_description,\n",
    "        \"headings\": headings,\n",
    "        \"paragraphs\": paragraphs\n",
    "    }\n",
    "    return extracted_data\n",
    "\n",
    "def classifier_model(classifier):\n",
    "    \"\"\"\n",
    "    This function loads the summarization and classification models.\n",
    "    Args:\n",
    "        classifier (str): The name of the zero-shot classification model to load.\n",
    "    Returns:\n",
    "        the loaded classification model.\n",
    "    \"\"\"\n",
    "     # Load the classifier model\n",
    "    classifier = pipeline(\"zero-shot-classification\", model=classifier, device=-1)\n",
    "\n",
    "    return classifier\n",
    "\n",
    "def summarize_model(summirizer):\n",
    "    \"\"\"\n",
    "    This function loads the summarization and classification models.\n",
    "    Args:\n",
    "        summirizer (str): The name of the summarization model to load.\n",
    "    Returns:\n",
    "        tuple: A tuple containing the loaded summarization model, tokenizer and device.\n",
    "    \"\"\"\n",
    "    # Set the device to GPU if available\n",
    "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load the summarization model and tokenizer\n",
    "    model = AutoModelForCausalLM.from_pretrained(summirizer).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(summirizer)\n",
    "\n",
    "\n",
    "    return model, tokenizer, device\n",
    "\n",
    "def classify_text(Text, classifier):\n",
    "    #docstring\n",
    "    \"\"\"\n",
    "    This function classifies the useful parts of the text data.\n",
    "    Args:\n",
    "        Text (list): A list of paragraphs to classify.\n",
    "        classifier (pipeline): The zero-shot classification pipeline.\n",
    "    Returns:\n",
    "        dict: A dictionary containing the classification results.\n",
    "    \"\"\"\n",
    "    labels = [\"useful\", \"not useful\"]\n",
    "    if not Text:\n",
    "        raise ValueError(\"The paragraphs list is empty. Please provide text data to classify.\")\n",
    "\n",
    "    # Predict useful parts\n",
    "    useful_paragraphs = []\n",
    "    for paragraph in Text:\n",
    "        if paragraph.strip():  # Ensure the paragraph is not empty or just whitespace\n",
    "            result = classifier(paragraph, candidate_labels=labels)\n",
    "            if result['labels'][0] == \"useful\":\n",
    "                useful_paragraphs.append(paragraph)\n",
    "        else:\n",
    "            print(\"Skipping empty paragraph.\")\n",
    "    return useful_paragraphs\n",
    "\n",
    "class EndSequenceStoppingCriteria(StoppingCriteria):\n",
    "    \"\"\"\n",
    "    This class defines a stopping criteria that checks if the generated text ends with a specific sequence.\n",
    "    Args:\n",
    "        stop_sequence (str): The sequence that the generated text should end with.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): The tokenizer used to decode the generated text.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stop_sequence, tokenizer):\n",
    "        self.stop_sequence = stop_sequence\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        # Decode the generated tokens to text\n",
    "        generated_text = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "        # Check if the stop sequence is in the generated text and the JSON structure is completed\n",
    "        if self.stop_sequence in generated_text and generated_text.endswith(self.stop_sequence):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "def prep_prompt(input_text, stop_sequence, device, tokenizer):\n",
    "    #docstring\n",
    "    \"\"\"\n",
    "    This function prepares the input text and stopping criteria for the model.\n",
    "    Args:\n",
    "        input_text (str): The input text to generate from.\n",
    "        stop_sequence (str): The sequence that the generated text should end with.\n",
    "        device (torch.device): The device to run the model on.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): The tokenizer used to encode the input text.\n",
    "    Returns:\n",
    "        tuple: A tuple containing the inputs and stopping criteria.\n",
    "    \"\"\"\n",
    "\n",
    "    # Encode the stop sequence\n",
    "    stopping_criteria = StoppingCriteriaList([EndSequenceStoppingCriteria(stop_sequence, tokenizer)])\n",
    "\n",
    "    # Set pad_token_id to eos_token_id if not already set\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        \n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Move inputs to the appropriate device\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    return inputs, stopping_criteria\n",
    "\n",
    "def generate_text(model, inputs, stopping_criteria, tokenizer):\n",
    "    \"\"\"\n",
    "    This function generates text using the model and input text.\n",
    "    Args:\n",
    "        model (transformers.PreTrainedModel): The model to generate text with.\n",
    "        inputs (dict): The input text encoded as input_ids and attention_mask.\n",
    "        stopping_criteria (StoppingCriteria): The stopping criteria to use during generation.\n",
    "    Returns:\n",
    "        str: The generated text.\n",
    "    \"\"\"\n",
    "    # Generate output with attention mask\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=1500,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        top_k=50, \n",
    "        top_p=0.95, \n",
    "        temperature=0.2,\n",
    "        stopping_criteria=stopping_criteria  # Add this line\n",
    "        \n",
    "    )\n",
    "    return outputs\n",
    "\n",
    "def summary(outputs, start_marker, end_marker, tokenizer):\n",
    "    \"\"\"\n",
    "    This function extracts the JSON part from the generated text.\n",
    "    Args:\n",
    "        outputs (torch.Tensor): The generated text output.\n",
    "        start_marker (str): The start marker for the JSON part.\n",
    "        end_marker (str): The end marker for the JSON part.\n",
    "    Returns:\n",
    "        str: The extracted JSON part.\n",
    "    \"\"\"\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Find the second occurrence of the start marker\n",
    "    first_start_index = generated_text.find(start_marker)\n",
    "    second_start_index = generated_text.find(start_marker, first_start_index + len(start_marker))\n",
    "\n",
    "    # Extract the JSON part from the generated text\n",
    "    start_index = second_start_index + len(start_marker)\n",
    "    end_index = generated_text.find(end_marker, start_index)\n",
    "    json_output = generated_text[start_index:end_index].strip()\n",
    "    return json_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rodri.RODRIGO\\OneDrive\\Desktop\\Codigos VSC\\GIT Hub\\LLM-Webscrapper\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the classifier model\n",
    "classifier = classifier_model(\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rodri.RODRIGO\\OneDrive\\Desktop\\Codigos VSC\\GIT Hub\\LLM-Webscrapper\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1326: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  return t.to(\n"
     ]
    }
   ],
   "source": [
    "#load the summarization model in a separete cell to avoid kernel crash\n",
    "model, tokenizer, device = summarize_model(\"meta-llama/Llama-3.2-1B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the company website to prospect\n",
    "url = \"https://growthxlabs.com/\"\n",
    "\n",
    "# Read the company's website\n",
    "extracted_data = read_company_site(url)\n",
    "# Classify the useful parts of the text\n",
    "useful_paragraphs = classify_text(extracted_data[\"paragraphs\"], classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the prompt\n",
    "input_text = f\"\"\"\n",
    "You are an assistant that provides concise company information. For each company, you will determine some attributes based on the provided information.\n",
    "In this regard, you will follow these rules:\n",
    "1. You will strictly follow the instructions described here.\n",
    "2. You will determine the attributes as accurately as possible based on the provided information.\n",
    "3. You will provide the response for these attributes strictly in JSON format, in a single structure, considering the fields \\attribute\\ and \\value\\. Do not include the question, just respond with attribute and value.\n",
    "4. The attributes you should respond to are:\n",
    "   4.1 Company Name (attribute \\\"Company Name\"\\)\n",
    "   4.2 Website (attribute \\\"Website\"\\)\n",
    "   4.3 Target Audience (attribute \\\"Target Audience\"\\)\n",
    "   4.4 Industry (attribute \\\"Industry\"\\)\n",
    "   4.5 Type of Business (attribute \\\"Type of Business\"\\)\n",
    "   4.6 Size of the Company (attribute Number of employees: Less than 10 - \\Very Small\\, 11 to 50 - \\Small\\, 51 to 250 - \\Medium\\, more than 250 - \\Large\\, No information - \\\"Not Found\"\\)\n",
    "   4.7 Location (attribute \\\"Location\"\\)\n",
    "   4.8 Contact Information (attribute \\\"Contact Information\"\\)\n",
    "   4.9 Summary (attribute \\\"Summary\"\\)\n",
    "5. If you cannot determine an attribute, or it has low probability, you will respond with the attribute and the value \\\"Not Found\"\\.\n",
    "6. The first field should be Attribute: \\\"Company Name\\\", value: the value will be provided along with the information.\n",
    "7. Example:\n",
    "    7.1 Before the JSON format you will respond with ###START###\n",
    "    7.2 The JSON format will be as follows:\n",
    "   {{\n",
    "       \"Company Name\": \"Company Name\",\n",
    "       \"Website\": \"Company URL\",\n",
    "       \"Target Audience\": \"Target Audience\",\n",
    "       \"Industry\": \"Industry\",\n",
    "       \"Type of Business\": \"Type of Business\"\n",
    "        \"Size of the Company\": \"Size of the Company\",\n",
    "        \"Location\": \"Location\",\n",
    "        \"Contact Information\": \"Contact Information\",\n",
    "        \"Summary\": \"Summary\"\n",
    "   }}\n",
    "    7.3 After generating the JSON, you will respond with ###END###\n",
    "8. Company information:\n",
    "    8.1 Website title - {extracted_data[\"title\"]}, \n",
    "    8.2 website url - {url} \n",
    "    8.3 Usefull text - {useful_paragraphs}\n",
    "9. Provide the requested information for the company.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Company Name\": \"GrowthX Labs\",\n",
      "    \"Website\": \"https://growthxlabs.com/\",\n",
      "    \"Target Audience\": \"Businesses and teams looking to scale their AI-powered growth strategies\",\n",
      "    \"Industry\": \"Technology\",\n",
      "    \"Type of Business\": \"AI-Powered Growth Agency\",\n",
      "    \"Size of the Company\": \"Very Small\",\n",
      "    \"Location\": \"New York, NY\",\n",
      "    \"Contact Information\": \"info@growthxlabs.com\",\n",
      "    \"Summary\": \"We help teams build end-to-end AI-powered, human-guided automated content workflows that actually drive growth.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Defining a stopping criteria\n",
    "stop_sequence = \"###END###\"\n",
    "\n",
    "# preparing the prompt to model\n",
    "inputs, stopping_criteria = prep_prompt(input_text, stop_sequence, device, tokenizer)\n",
    "\n",
    "# summary of the company\n",
    "outputs = generate_text(model, inputs, stopping_criteria, tokenizer)\n",
    "\n",
    "# Extract the JSON part from the generated text\n",
    "start_marker = \"###START###\"\n",
    "end_marker = \"###END###\"\n",
    "json_output = summary(outputs, start_marker, end_marker, tokenizer)\n",
    "\n",
    "# Print the extracted JSON output - using this just as a test. In a production environment, this will either be sent to a database or an e-mail\n",
    "print(json_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
