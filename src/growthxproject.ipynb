{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "import torch\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_company_site(url):\n",
    "\n",
    "    \"\"\"\n",
    "    This function reads the content of a company's website and extracts the title, meta description, headings, and paragraphs.\n",
    "    Args:\n",
    "        url (str): The URL of the company's website.\n",
    "    Returns:\n",
    "        dict: A dictionary containing the extracted data.\n",
    "    \"\"\"\n",
    "    response = requests.get(url, timeout=5)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Extract various parts of the webpage\n",
    "    title = soup.find('title').get_text(strip=True) if soup.find('title') else \"No title found\"\n",
    "    meta_description = soup.find('meta', attrs={'name': 'description'})\n",
    "    meta_description = meta_description['content'].strip() if meta_description else \"No description found\"\n",
    "    headings = [h.get_text(strip=True) for h in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])]\n",
    "    paragraphs = [p.get_text(strip=True) for p in soup.find_all('p')]\n",
    "\n",
    "    # Combine extracted parts into a dictionary\n",
    "    extracted_data = {\n",
    "        \"title\": title,\n",
    "        \"meta_description\": meta_description,\n",
    "        \"headings\": headings,\n",
    "        \"paragraphs\": paragraphs\n",
    "    }\n",
    "    return extracted_data\n",
    "\n",
    "def classifier_model(classifier):\n",
    "    \"\"\"\n",
    "    This function loads the summarization and classification models.\n",
    "    Args:\n",
    "        classifier (str): The name of the zero-shot classification model to load.\n",
    "    Returns:\n",
    "        the loaded classification model.\n",
    "    \"\"\"\n",
    "     # Load the classifier model\n",
    "    classifier = pipeline(\"zero-shot-classification\", model=classifier, device=-1)\n",
    "\n",
    "    return classifier\n",
    "\n",
    "def summarize_model(summirizer):\n",
    "    \"\"\"\n",
    "    This function loads the summarization and classification models.\n",
    "    Args:\n",
    "        summirizer (str): The name of the summarization model to load.\n",
    "    Returns:\n",
    "        tuple: A tuple containing the loaded summarization model, tokenizer and device.\n",
    "    \"\"\"\n",
    "    # Set the device to GPU if available\n",
    "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load the summarization model and tokenizer\n",
    "    model = AutoModelForCausalLM.from_pretrained(summirizer).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(summirizer)\n",
    "\n",
    "\n",
    "    return model, tokenizer, device\n",
    "\n",
    "def classify_text(Text, classifier):\n",
    "    #docstring\n",
    "    \"\"\"\n",
    "    This function classifies the useful parts of the text data.\n",
    "    Args:\n",
    "        Text (list): A list of paragraphs to classify.\n",
    "        classifier (pipeline): The zero-shot classification pipeline.\n",
    "    Returns:\n",
    "        dict: A dictionary containing the classification results.\n",
    "    \"\"\"\n",
    "    labels = [\"useful\", \"not useful\"]\n",
    "    if not Text:\n",
    "        raise ValueError(\"The paragraphs list is empty. Please provide text data to classify.\")\n",
    "\n",
    "    # Predict useful parts\n",
    "    useful_paragraphs = []\n",
    "    for paragraph in Text:\n",
    "        if paragraph.strip():  # Ensure the paragraph is not empty or just whitespace\n",
    "            result = classifier(paragraph, candidate_labels=labels)\n",
    "            if result['labels'][0] == \"useful\":\n",
    "                useful_paragraphs.append(paragraph)\n",
    "        else:\n",
    "            print(\"Skipping empty paragraph.\")\n",
    "    return useful_paragraphs\n",
    "\n",
    "class EndSequenceStoppingCriteria(StoppingCriteria):\n",
    "    \"\"\"\n",
    "    This class defines a stopping criteria that checks if the generated text ends with a specific sequence.\n",
    "    Args:\n",
    "        stop_sequence (str): The sequence that the generated text should end with.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): The tokenizer used to decode the generated text.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stop_sequence, tokenizer):\n",
    "        self.stop_sequence = stop_sequence\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        # Decode the generated tokens to text\n",
    "        generated_text = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "        # Check if the stop sequence is in the generated text and the JSON structure is completed\n",
    "        if self.stop_sequence in generated_text and generated_text.endswith(self.stop_sequence):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "def prep_prompt(input_text, stop_sequence, device, tokenizer):\n",
    "    #docstring\n",
    "    \"\"\"\n",
    "    This function prepares the input text and stopping criteria for the model.\n",
    "    Args:\n",
    "        input_text (str): The input text to generate from.\n",
    "        stop_sequence (str): The sequence that the generated text should end with.\n",
    "        device (torch.device): The device to run the model on.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): The tokenizer used to encode the input text.\n",
    "    Returns:\n",
    "        tuple: A tuple containing the inputs and stopping criteria.\n",
    "    \"\"\"\n",
    "\n",
    "    # Encode the stop sequence\n",
    "    stopping_criteria = StoppingCriteriaList([EndSequenceStoppingCriteria(stop_sequence, tokenizer)])\n",
    "\n",
    "    # Set pad_token_id to eos_token_id if not already set\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        \n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Move inputs to the appropriate device\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    return inputs, stopping_criteria\n",
    "\n",
    "def generate_text(model, inputs, stopping_criteria, tokenizer):\n",
    "    \"\"\"\n",
    "    This function generates text using the model and input text.\n",
    "    Args:\n",
    "        model (transformers.PreTrainedModel): The model to generate text with.\n",
    "        inputs (dict): The input text encoded as input_ids and attention_mask.\n",
    "        stopping_criteria (StoppingCriteria): The stopping criteria to use during generation.\n",
    "    Returns:\n",
    "        str: The generated text.\n",
    "    \"\"\"\n",
    "    # Generate output with attention mask\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=2500,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        top_k=50, \n",
    "        top_p=0.95, \n",
    "        num_beams=2,\n",
    "        temperature=0.3,\n",
    "        stopping_criteria=stopping_criteria  # Add this line\n",
    "        \n",
    "    )\n",
    "    return outputs\n",
    "\n",
    "def summary(outputs, start_marker, end_marker, tokenizer):\n",
    "    \"\"\"\n",
    "    This function extracts the JSON part from the generated text.\n",
    "    Args:\n",
    "        outputs (torch.Tensor): The generated text output.\n",
    "        start_marker (str): The start marker for the JSON part.\n",
    "        end_marker (str): The end marker for the JSON part.\n",
    "    Returns:\n",
    "        str: The extracted JSON part.\n",
    "    \"\"\"\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Find the second occurrence of the start marker\n",
    "    first_start_index = generated_text.find(start_marker)\n",
    "    second_start_index = generated_text.find(start_marker, first_start_index + len(start_marker))\n",
    "\n",
    "    # Extract the JSON part from the generated text\n",
    "    start_index = second_start_index + len(start_marker)\n",
    "    end_index = generated_text.find(end_marker, start_index)\n",
    "    json_output = generated_text[start_index:end_index].strip()\n",
    "    return json_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rodri.RODRIGO\\OneDrive\\Desktop\\Codigos VSC\\GIT Hub\\LLM-Webscrapper\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the classifier model\n",
    "classifier = classifier_model(\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rodri.RODRIGO\\OneDrive\\Desktop\\Codigos VSC\\GIT Hub\\LLM-Webscrapper\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1326: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  return t.to(\n"
     ]
    }
   ],
   "source": [
    "#load the summarization model in a separete cell to avoid kernel crash\n",
    "model, tokenizer, device = summarize_model(\"meta-llama/Llama-3.2-1B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the company website to prospect\n",
    "url = \"https://datahubproject.io/\"\n",
    "\n",
    "# Read the company's website\n",
    "extracted_data = read_company_site(url)\n",
    "# Classify the useful parts of the text\n",
    "useful_paragraphs = classify_text(extracted_data[\"paragraphs\"], classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the prompt\n",
    "input_text = f\"\"\"\n",
    "You are an assistant that provides concise company information. For each company, you will determine some attributes based on the provided information.\n",
    "In this regard, you will follow these rules:\n",
    "1. You will strictly follow the instructions described here.\n",
    "2. You will determine the attributes as accurately as possible based on the provided information.\n",
    "3. You will provide the response for these attributes strictly in JSON format, in a single structure, considering the fields \\attribute\\ and \\value\\. Do not include the question, just respond with attribute and value.\n",
    "4. The attributes you should respond to are:\n",
    "   4.1 Company Name (attribute \\\"Company Name\"\\)\n",
    "   4.2 Website (attribute \\\"Website\"\\)\n",
    "   4.3 Target Audience (attribute \\\"Target Audience\"\\)\n",
    "   4.4 Industry (attribute \\\"Industry\"\\)\n",
    "   4.5 Type of Business (attribute \\\"Type of Business\"\\)\n",
    "   4.6 Size of the Company (attribute Number of employees: Less than 1 - \\\"Not Found\"\\, more than 250 - \\\"Large\"\\, 51 to 250 - \\\"Medium\"\\, 11 to 50 - \\\"Small\"\\, 1 - 10 - \\\"Very Small\"\\ )\n",
    "   4.7 Location (attribute \\\"Location\"\\)\n",
    "   4.8 Contact Information (attribute \\\"E-mail\"\\)\n",
    "   4.9 Summary (attribute \\\"Summary\"\\)\n",
    "5. If you cannot determine an attribute, or it has low probability, you will respond with the attribute and the value \\\"Not Found\"\\.\n",
    "6. The first field should be Attribute: \\\"Company Name\\\", value: the value will be provided along with the information.\n",
    "7. Example:\n",
    "    7.1 Before the JSON format you will respond with ###START###\n",
    "    7.2 The JSON format will be as follows:\n",
    "   {{\n",
    "       \"Company Name\": \"Company Name\",\n",
    "       \"Website\": \"Company URL\",\n",
    "       \"Target Audience\": \"Target Audience\",\n",
    "       \"Industry\": \"Industry\",\n",
    "       \"Type of Business\": \"Type of Business\"\n",
    "        \"Size of the Company\": \"Size of the Company\",\n",
    "        \"Location\": \"Location\",\n",
    "        \"Contact Information\": \"Contact Information\",\n",
    "        \"Summary\": \"Summary\"\n",
    "   }}\n",
    "    7.3 After generating the JSON, you will respond with ###END###\n",
    "8. Company information:\n",
    "    8.1 Website title - {extracted_data[\"title\"]}, \n",
    "    8.2 website url - {url} \n",
    "    8.3 Usefull text - {useful_paragraphs}\n",
    "9. Provide the requested information for the company.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Company Name\": \"DataHub\",\n",
      "    \"Website\": \"https://datahubproject.io/\",\n",
      "    \"Target Audience\": \"General Public\",\n",
      "    \"Industry\": \"Data Science and Analytics\",\n",
      "    \"Type of Business\": \"Software as a Service\",\n",
      "    \"Size of the Company\": \"Large\",\n",
      "    \"Location\": \"San Francisco, CA, USA\",\n",
      "    \"Contact Information\": \"support@datahubproject.io\",\n",
      "    \"Summary\": \"The #1 Open Source Metadata Platform | DataHub\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Defining a stopping criteria\n",
    "stop_sequence = \"###END###\"\n",
    "\n",
    "# preparing the prompt to model\n",
    "inputs, stopping_criteria = prep_prompt(input_text, stop_sequence, device, tokenizer)\n",
    "\n",
    "# summary of the company\n",
    "outputs = generate_text(model, inputs, stopping_criteria, tokenizer)\n",
    "\n",
    "# Extract the JSON part from the generated text\n",
    "start_marker = \"###START###\"\n",
    "end_marker = \"###END###\"\n",
    "json_output = summary(outputs, start_marker, end_marker, tokenizer)\n",
    "\n",
    "# Print the extracted JSON output - using this just as a test. In a production environment, this will either be sent to a database or an e-mail\n",
    "print(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned JSON string: {\n",
      "    \"Company Name\": \"Unilever Global\",\n",
      "    \"Website\": \"https://www.unilever.com/\",\n",
      "    \"Target Audience\": \"Consumer Goods\",\n",
      "    \"Industry\": \"Consumer Goods\",\n",
      "    \"Type of Business\": \"Consumer Goods\",\n",
      "    \"Size of the Company\": \"Large\",\n",
      "    \"Location\": \"Global\",\n",
      "    \"Contact Information\": \"E-mail: [info@unilever.com](mailto:info@unilever.com)\",\n",
      "    \"Summary\": \"Unilever Global is a global consumer goods business with strong fundamentals and differentiated capabilities. We are stepping up our execution to deliver improved performance.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import json\n",
    "import re\n",
    "# Initialize an empty DataFrame\n",
    "columns = [\"Company Name\", \"Website\", \"Target Audience\", \"Industry\", \"Type of Business\", \"Size of the Company\", \"Location\", \"Contact Information\", \"Summary\"]\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Assuming json_output is a JSON string\n",
    "json_output1 = \"\"\"\n",
    "{\n",
    "    \"Company Name\": \"Unilever Global\",\n",
    "    \"Website\": \"https://www.unilever.com/\",\n",
    "    \"Target Audience\": \"Global consumers\",\n",
    "    \"Industry\": \"Consumer Goods\",\n",
    "    \"Type of Business\": \"Multinational\",\n",
    "    \"Size of the Company\": \"Very Small\",\n",
    "    \"Location\": \"Global\",\n",
    "    \"Contact Information\": \"Unilever PLC, 1-2 Newlands Road, London SW1A 1AA, UK\",\n",
    "    \"Summary\": \"Unilever is a global consumer goods company with a diverse portfolio of brands, including Axe, Dove, and Knorr.\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "json_match = re.search(r'\\{.*\\}', json_output, re.DOTALL)\n",
    "if json_match:\n",
    "    json_str = json_match.group(0)\n",
    "    # Clean the JSON string\n",
    "    json_str = re.sub(r',\\s*}', '}', json_str)\n",
    "    # Debugging: Print the cleaned JSON string\n",
    "    print(\"Cleaned JSON string:\", json_str)\n",
    "else:\n",
    "    print(\"No JSON object found in company_info\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Company Name\": \"Unilever Global\",\n",
      "    \"Website\": \"https://www.unilever.com/\",\n",
      "    \"Target Audience\": \"Consumer Goods\",\n",
      "    \"Industry\": \"Consumer Goods\",\n",
      "    \"Type of Business\": \"Consumer Goods\",\n",
      "    \"Size of the Company\": \"Large\",\n",
      "    \"Location\": \"Global\",\n",
      "    \"Contact Information\": \"E-mail: [info@unilever.com](mailto:info@unilever.com)\",\n",
      "    \"Summary\": \"Unilever Global is a global consumer goods business with strong fundamentals and differentiated capabilities. We are stepping up our execution to deliver improved performance.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "print(re.sub(r',\\s*}', '}', json_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an assistant that provides concise company information. For each company, you will determine some attributes based on the provided information.\n",
      "In this regard, you will follow these rules:\n",
      "1. You will strictly follow the instructions described here.\n",
      "2. You will determine the attributes as accurately as possible based on the provided information.\n",
      "3. You will provide the response for these attributes strictly in JSON format, in a single structure, considering the fields \u0007ttribute\\ and \u000balue\\. Do not include the question, just respond with attribute and value.\n",
      "4. The attributes you should respond to are:\n",
      "   4.1 Company Name (attribute \"Company Name\"\\)\n",
      "   4.2 Website (attribute \"Website\"\\)\n",
      "   4.3 Target Audience (attribute \"Target Audience\"\\)\n",
      "   4.4 Industry (attribute \"Industry\"\\)\n",
      "   4.5 Type of Business (attribute \"Type of Business\"\\)\n",
      "   4.6 Size of the Company (attribute Number of employees: Less than 1 - \"Not Found\"\\, more than 250 - \"Large\"\\, 51 to 250 - \"Medium\"\\, 11 to 50 - \"Small\"\\, 1 - 10 - \"Very Small\"\\ )\n",
      "   4.7 Location (attribute \"Location\"\\)\n",
      "   4.8 Contact Information (attribute \"E-mail\"\\)\n",
      "   4.9 Summary (attribute \"Summary\"\\)\n",
      "5. If you cannot determine an attribute, or it has low probability, you will respond with the attribute and the value \"Not Found\"\\.\n",
      "6. The first field should be Attribute: \"Company Name\", value: the value will be provided along with the information.\n",
      "7. Example:\n",
      "    7.1 Before the JSON format you will respond with ###START###\n",
      "    7.2 The JSON format will be as follows:\n",
      "   {\n",
      "       \"Company Name\": \"Company Name\",\n",
      "       \"Website\": \"Company URL\",\n",
      "       \"Target Audience\": \"Target Audience\",\n",
      "       \"Industry\": \"Industry\",\n",
      "       \"Type of Business\": \"Type of Business\"\n",
      "        \"Size of the Company\": \"Size of the Company\",\n",
      "        \"Location\": \"Location\",\n",
      "        \"Contact Information\": \"Contact Information\",\n",
      "        \"Summary\": \"Summary\"\n",
      "   }\n",
      "    7.3 After generating the JSON, you will respond with ###END###\n",
      "8. Company information:\n",
      "    8.1 Website title - Unilever Global: Making sustainable living commonplace | Unilever, \n",
      "    8.2 website url - https://www.unilever.com/ \n",
      "    8.3 Usefull text - ['Unilever GlobalChange location', 'We are a global consumer goods business with strong fundamentals and differentiated capabilities.', 'We are stepping up our execution to deliver improved performance.', 'Our international leadership team drives our company, guides our strategy and leads our people.', 'Unilever has a unique heritage which still shapes the way we do business today.', 'View our latest news, search and browse by topic and date.', 'See the very latest from our press desk, plus media assets and contact details.', 'Be the first to hear about our press releases, features and articles.', 'Led by our 30 Power Brands, our brands are meeting consumers’ daily needs, from household staples to premium indulgence.', 'Every one of our products is developed using world-class science and technology.', 'Discover the ingredients in our home, beauty and personal care products.', 'The world must move further faster to avoid the worst effects of climate change.', 'We’re committed to contributing to the protection and regeneration of nature, within and beyond our value chain.', 'Given the size of the plastics challenge, we’re using our innovation capabilities to find new, scalable solutions.', 'We’re working to improve the livelihoods of people in our global value chain.', 'Our progress and reporting on issues such as human rights, equity, diversity and inclusion and safety.', 'We want to work with partners who share our purpose and values. See what’s involved.', 'Our Partner to Win programme is taking our partnerships to a whole new level.', 'Together with our partners, we’re helping to provide decent livelihoods and supporting inclusive economic growth.', \"We're collaborating with key suppliers to reduce emissions from raw materials, ingredients and packaging by 2030, which will help us progress towards our net zero ambition.\", '7 November 2024', '6 November 2024', '4 November 2024', '31 October 2024', '30 October 2024', '28 October 2024', 'Our latest share informationfor Unilever PLC', 'Unilever released its Third quarter Trading Statement on 24 October 2024.', 'There was a webcast at 8:00 am (UK time) and a replay is now available.', 'JavaScript must be enabled to view this content', 'Climate action has long been a part of how we do business. But the world must move faster to avoid the worst effects of climate change. We’ve set more ambitious climate targets and identified clearer actions to respond to this challenge.', 'The world – and our business – needs resilient natural and agricultural ecosystems to thrive. We’re committed to contributing to the protection and regeneration of nature, within and beyond our value chain.', 'We’ve been working hard to create a circular economy for plastic packaging for a number of years. We’ve learnt that transformation takes time. Given the size of this challenge, we’re using our innovation capabilities to find new, scalable solutions.', 'The impacts of inequality go far beyond income – to health, human rights and economic growth. So we’re working to improve the livelihoods of people in our global value chain.', '3.4 billion peoplein 190 countriesuse our brands every day.', \"We're always looking to connect with those who share an interest in a sustainable future.\", 'Get in touch with Unilever PLC and specialist teams in our headquarters, or find contacts around the world.', \"This is Unilever's global company website\", '© Unilever 2024']\n",
      "9. Provide the requested information for the company.\n",
      "###START###\n",
      "{\n",
      "    \"Company Name\": \"Unilever\",\n",
      "    \"Website\": \"https://www.unilever.com/\",\n",
      "    \"Target Audience\": \"Consumer Goods\",\n",
      "    \"Industry\": \"Consumer Goods\",\n",
      "    \"Type of Business\": \"Consumer Goods\",\n",
      "    \"Size of the Company\": \"Large\",\n",
      "    \"Location\": \"Global\",\n",
      "    \"Contact Information\": \"E-mail: [info@unilever.com](mailto:info@unilever.com)\",\n",
      "    \"Summary\": \"Unilever is a global consumer goods business with strong fundamentals and differentiated capabilities.\",\n",
      "    \"Industry\": \"Consumer Goods\",\n",
      "    \"Type of Business\": \"Consumer Goods\"\n",
      "}\n",
      "###END###\n",
      "\n",
      "###END###\n",
      "The JSON format you requested is not suitable for this response. Here is the correct JSON format for the provided information:\n",
      "{\n",
      "    \"Company Name\": \"Unilever\",\n",
      "    \"Website\": \"https://www.unilever.com/\",\n",
      "    \"Target Audience\": \"Consumer Goods\",\n",
      "    \"Industry\": \"Consumer Goods\",\n",
      "    \"Type of Business\": \"Consumer Goods\",\n",
      "    \"Size of the Company\": \"Large\",\n",
      "    \"Location\": \"Global\",\n",
      "    \"Contact Information\": \"E-mail: [info@unilever.com](mailto:\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
